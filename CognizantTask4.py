# -*- coding: utf-8 -*-
"""CognizantTask4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xL1BcxO5Z2wSdZmmY-ynp4c2ado31NrH

# Code Challenge - Task 4: Create Python Module

***
*Author: Kadriye Nur Bakirci*
***
Contact regarding the code: nur.bakirci@gmail.com
***

This module includes several functions to perform regression task.

[Datasets Location](https://github.com/kadnur/DataScienceProject/tree/main/Data)
"""

# Import important libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
import numpy as np
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# Load data from github repository
def load_data(dataset_name):
    """
    This function takes name of CSV file and loads it into
    a Pandas DataFrame.

    :param      name of CSV file

    :return     df: pd.DataFrame
    """
    # Clone the repo containing the raw data
    !git clone -l -s https://github.com/kadnur/DataScienceProject.git cloned-repo

    # Move to the relative path containing the cloned repo's data
#     %cd cloned-repo/Data
    !ls

    # Read CSV file
    df = pd.read_csv(dataset_name)
    return df

# Create target variable and predictor variables
def create_target_and_predictors(
    data: pd.DataFrame = None,
    target: str = "estimated_stock_pct"
):
    """
    This function takes in a Pandas DataFrame and splits the columns
    into a target column and a set of predictor variables, i.e. X & y.
    These two splits of the data will be used to train a supervised
    machine learning model.

    :param      data: pd.DataFrame, dataframe containing data for the
                      model
    :param      target: str (optional), target variable that you want to predict

    :return     X: pd.DataFrame
                y: pd.Series
    """

    # Check to see if the target variable is present in the data
    if target not in data.columns:
        raise Exception(f"Target: {target} is not present in the data")

    X = data.drop(columns=[target])
    y = data[target]
    return X, y

# Train algorithm
def train_algorithm_with_cross_validation(
    X: pd.DataFrame = None,
    y: pd.Series = None,
    K: int = None,
    Split: float = None
):
    """
    This function takes the predictor and target variables and
    trains a Random Forest Regressor model across K folds. Using
    cross-validation, performance metrics will be output for each
    fold during training and also it plots the most important features.

    :param      X: pd.DataFrame, predictor variables
    :param      y: pd.Series, target variable
    :param      K: integer to define number of folds
    :param      Split: float to define the size of train dataset

    :return
    """

    # Create a list that will store the accuracies of each fold for both train and test.
    accuracy_test = []
    accuracy_train = []

    # Enter a loop to run K folds of cross-validation
    for fold in range(0, K):

        # Instantiate algorithm and scaler
        model = RandomForestRegressor()
        scaler = StandardScaler()

        # Create training and test samples
        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=Split, random_state=42)

        # Scale X data, we scale the data because it helps the algorithm to converge
        # and helps the algorithm to not be greedy with large values
        scaler.fit(X_train)
        X_train_scaled = scaler.transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Train model on training data
        trained_model = model.fit(X_train_scaled, y_train)

        # Generate predictions on training and test samples
        y_pred_train = trained_model.predict(X_train_scaled)
        y_pred_test = trained_model.predict(X_test_scaled)

        # Compute MAE for training data
        mae_train = mean_absolute_error(y_true=y_train, y_pred=y_pred_train)
        accuracy_train.append(mae_train)
        print(f"Fold {fold + 1}: Training MAE = {mae_train:.3f}")

        # Compute MAE for test data
        mae_test = mean_absolute_error(y_true=y_test, y_pred=y_pred_test)
        accuracy_test.append(mae_test)
        print(f"Fold {fold + 1}: Test MAE = {mae_test:.3f}")

    # Finish by computing the average MAE across all folds
    print(f"Average Training MAE: {(sum(accuracy_train) / len(accuracy_train)):.2f}")
    print(f"Average Test MAE: {(sum(accuracy_test) / len(accuracy_test)):.2f}")

    features = [i.split("__")[0] for i in X.columns]
    importances = model.feature_importances_
    indices = np.argsort(importances)

    fig, ax = plt.subplots(figsize=(10, 20))
    plt.title('Feature Importances')
    plt.barh(range(len(indices)), importances[indices], color='b', align='center')
    plt.yticks(range(len(indices)), [features[i] for i in indices])
    plt.xlabel('Relative Importance')

def run():
    """
    This function executes the training pipeline of loading the prepared
    dataset from a CSV file and training the machine learning model

    :param

    :return
    """
  # Load data
    data = load_data("merged_df.csv")

    # Create target and predictors
    X, y = create_target_and_predictors(data, "estimated_stock_pct")

    # Train algorithm with cross-validation
    train_algorithm_with_cross_validation(X, y, 10, 0.8)